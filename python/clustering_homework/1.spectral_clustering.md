Spectral clustering
=

Spectral Clustering is a variant of the clustering algorithm that uses the 
connectivity between the data points to form the clustering. 
It uses eigenvalues and eigenvectors of the data matrix to forecast the 
data into lower dimensions space to cluster the data points. 
It is based on the idea of a graph representation of data where the data 
point are represented as nodes and the similarity between the data points 
are represented by an edge. 

GeeksforGeeks. (2023). ML | Spectral Clustering. GeeksforGeeks. 
https://www.geeksforgeeks.org/ml-spectral-clustering/

a. In which cases might it be more useful to apply?
-

Spectral clustering is particularly useful in situations where the 
groupings are not linearly separable or have irregular shapes :

some cases are:

Non-spherical clusters: Unlike algorithms like k-means, which tend to 
find spherical clusters, spectral clustering can identify clusters 
with non-convex or irregular shapes.

Clusters with varying densities: Spectral clustering can detect clusters 
that have different point densities, which can be challenging for 
distance-based methods like k-means.

Non-linear dimensionality reduction: Spectral clustering can also be 
used for non-linear dimensionality reduction, as the eigenvectors of 
the Laplacian matrix represent a projection of the data into a 
lower-dimensional space.

Image segmentation: Spectral clustering can be used to segment images 
based on the similarity of pixels, taking into account both color 
intensity and spatial location of the pixels. This can help identify 
regions of interest in images with complex or irregular shapes.

Text analysis and natural language processing (NLP): Spectral 
clustering can be applied in text analysis to group documents or 
words based on their semantic similarity. This can be useful for 
identifying common themes, organizing large collections of documents, 
or improving search and recommendation systems.

b. What are the mathematical fundamentals of it?
-

1. **Affinity Matrix (Similarity Matrix):** Spectral clustering starts 
with defining an affinity matrix (or similarity matrix), 
denoted as $W$, that encodes the pairwise similarity or distance 
between data points. Common choices for measuring similarity include 
Gaussian kernels, k-nearest neighbors, or other similarity metrics.

2. **Graph Representation:** The affinity matrix can also be thought 
of as the weighted adjacency matrix of a graph, where each data point 
corresponds to a node in the graph, and the weights on the edges 
between nodes represent the pairwise similarities.

3. **Degree Matrix:** The degree matrix, denoted as $D$, is a 
diagonal matrix where each diagonal element $D_{ii}$ represents the 
sum of the weights (similarities) for the $i$-th data point. 
Mathematically, $D_{ii} = \sum_j W_{ij}$.

4. **Normalized Laplacian Matrix:** The normalized Laplacian matrix, 
denoted as $L_{\text{norm}}$, is defined as 
$L_{\text{norm}} = I - D^{-1/2}WD^{-1/2}$, where $I$ is the 
identity matrix and $D^{-1/2}$ is the square root of the inverse 
degree matrix. The Laplacian matrix characterizes the structural 
properties of the graph.

5. **Eigenvectors and Eigenvalues:** Spectral clustering then involves 
computing the eigenvectors and eigenvalues of the normalized Laplacian 
matrix $L_{\text{norm}}$. These eigenvectors represent different 
modes of variation in the data.

6. **Dimension Reduction:** After computing the eigenvectors, you 
typically select a subset of them (often the ones corresponding to the 
smallest eigenvalues) to reduce the dimensionality of the data. 
This is similar to performing Principal Component Analysis (PCA) on 
the data.

7. **Clustering:** Finally, you perform traditional clustering 
algorithms (e.g., k-means) on the lower-dimensional representation of 
the data obtained from the selected eigenvectors. The number of 
clusters is often determined based on various criteria or heuristics.

c. What is the algorithm to compute it?
-

- Project data into $R^n$ matrix
- Define an Affinity matrix A , using a Gaussian Kernel K or an Adjacency matrix
- Construct the Graph Laplacian from A (i.e. decide on a normalization)
- Solve the Eigenvalue problem
- Select k eigenvectors corresponding to the k lowest (or highest) 
eigenvalues to define a k-dimensional subspace
- Form clusters in this subspace using k-means

d. Does it hold any relation to some of the concepts previously mentioned in class? Which, and how?
-
Spectral clustering employs similarity metrics like k-means 
and k-medoids to gauge the degree of similarity or dissimilarity 
among data points. It then leverages eigenvalues and eigenvectors to 
perform a transformation of the data, reducing it to a 
lower-dimensional space.